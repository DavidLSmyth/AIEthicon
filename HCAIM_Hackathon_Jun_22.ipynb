{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HCAIM-Hackathon-Jun-22",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "  <IMG SRC=\"https://humancentered-ai.eu/wp-content/uploads/2022/03/hcaim-nav-1.png\" WIDTH=250 ALIGN=\"right\">\n",
        "</figure>\n",
        "\n",
        "# Haggle: A Competitive Privacy-Accuracy Tradeoff! \n",
        "*Made by Oblivious, CeADAR, OpenDP with* ü´∂ for the **HCAIM Pioneers**.\n",
        "\n",
        "\n",
        "### Technology stack ‚öôÔ∏è\n",
        "\n",
        "The tech stack you'll get hands on experience with includes:\n",
        "\n",
        "[![](https://img.shields.io/badge/-OBLV:_Made%20for%20Enclaves-93228f?style=for-the-badge&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAQAAADZc7J%2FAAAABGdBTUEAALGPC%2FxhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA%2F4ePzL8AAAAJcEhZcwAADsMAAA7DAcdvqGQAAAAHdElNRQfmBRQLNCJ5TUB1AAACu0lEQVRIx42VTWhdVRDHf7mxJe%2BlTWqVSlKJbZCGIib1iSGLbCq4aKkQlJagATdulFKEQMStduHGEHCRViW6EVwJQhBCqaYgih8YkxQhUluhxGIDMU2TmPhe%2BnNx7n339eV9OHcxd2bO%2F3%2FnnJkzt4EKYiunOE43h9gHrPAHs3zNZMMq9cWjfuyGlWTDCbtqg7OOmbeW5B01Uw3e7g%2F%2BH5mxoxI855%2Fpmiu%2BYa8PibjfPof9xntpeNFcOfzRFP65T0qF55iTKcVtj5TCM34f%2FHc8LeIuB%2FzAb73mb172XZ%2BOSV72bkJx1WxKMBZ8S%2FaIeMrfd2x8Oo4943LiOp8WLq%2B6aZ%2BII6W7LZEtXxGx363gWLc9EHwS7GERB2sc%2F7ZnRHwrcVwEbAlts%2BADYrN%2F1azgigfEXV5Lctgb8TwZgPcoAEMcqNlqrbwG5BkNZpaT%2BJHqv%2B4XcapuF03HvRE37MWIHoCfWQYgRz1pBWCZmWD2RBwCuApAEw%2FXJViI9a9BHY5oAbgNwO66cPgw1rfihKKg7wHwD9aBX%2BBy%2FFaIdcQqwIMA5FmqAZb3OVu04s3eibgBkNyM%2Barwm5zgHNtF%2B2hQ1yNmAfpoAuBSFfg0TzFVYmfoDS9z%2BFIoaGjSxyxUqP2PZsqu9lASOoMtrqv%2BZCTiZxUIesvgkTNJK%2B8BnAjWqyJ2uFIG39gxWl5PQuPhdLtCX655TMSBsm0UbL4Pngsp65ptSYFGg%2BeWT4j4Yjp3Ss4nPN3pfX07rXBTMtL%2B9gURO%2F3UzSLBcBE%2B6GrinC8ZaeBBF5PIF%2FHwarbf0z5nZzH1%2B4bq4%2BV9lnMpjX%2FniP0%2BYqONttnvm0mKCbzStfWI83XHgercjq8XKbKeT464iqz5TtVfW0zS7oUqJOuOFwtXlIaKJHs5ybP00EkrsMINfuErvmxY27n2P7cF8FI7UOqVAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA1LTIwVDExOjUyOjM0KzAwOjAwh4HVcwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wNS0yMFQxMTo1MjozNCswMDowMPbcbc8AAAAZdEVYdFNvZnR3YXJlAHd3dy5pbmtzY2FwZS5vcmeb7jwaAAAAV3pUWHRSYXcgcHJvZmlsZSB0eXBlIGlwdGMAAHic4%2FIMCHFWKCjKT8vMSeVSAAMjCy5jCxMjE0uTFAMTIESANMNkAyOzVCDL2NTIxMzEHMQHy4BIoEouAOoXEXTyQjWVAAAAAElFTkSuQmCC)](https://oblivious.ai) [![](https://img.shields.io/badge/-OpenDP_SmartNoise-blue?style=for-the-badge&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAACMAAAAjCAYAAAAe2bNZAAAHlklEQVR42p1YA7CkzQ69Wtu2bdu2bdu2bdu2bdu2bQ7yclI3r1Lfr91N1ZnP6dNRp8fn34SIfBl%2B9l7Dhg1bNWrUaM2bN29mEtF%2BxhXGecYlxknGCkZ7RkFGeKPLD%2Fp8%2FkQMCZyH%2FvnzZ3sMvnnz5s%2B49fLlS%2FoFucAYyohrdPn%2FLhF%2Fc16bcVu1P3v2jNauXev5%2FPmzm4g8DC8Exy9fvninTJni2bdvn4tv4bnKG0Y%2FRrjfIkREAYHHqIyVRiEGdmHQBw8eCClwUEAeP35MESNGJLYeQQIJASpwZc5fIaQv4JiWcZ0BEQtgQI%2FHQ5CRI0dSly5d6OvXrwRRMuxKunLlCr17944cghcwEch3RgUz8X8lkonxlAFx2cGUzJIlS6h69er0%2Fv17JSNg1wkhvWefGX16LG%2FHdQYrjtE0PliByxKwA7x69Ypu375N9pnL5aJx48bRhg0b8N6%2FEXIbtxezhCR9DakNgR%2F%2F1EHbt29Pp0%2BftspFNm7cSBcvXiSVt2%2FfElQsXrzYWpE%2BfvxIkydPprNnz6oOS%2BgRI5by%2BL9V3G53I2WugyImunfvTunSpaM7d%2B6QCr9LGTJkoKFDh9KPHz8I8vz5c2rVqhVduHCBrMyYMUNInjt3Tsk4XbbUaZ3QjHtqPv1IU7lIkSJUp04dshm0evVqpLgQw7VaAeT0euvWrSCC7HK6G%2FDqJbs4t4%2FK9%2B%2FfO3%2F48IG%2BffvmURMDGAiC2SJgr169SqqMa4qksloGWXT%2F%2Fn18h%2BfQRc2bN6dJkyZp1ukznKo11Tor1SohX7x4cRSpOnHiRJd%2BqAIFCE5UXF4CiN8lyOvXr6lx48ZifhArWbIkBrYWADnJOFsW9NvZs2dT4sSJvVwgpSqwZAaZXGwB7%2B7du71btmzxIjVv3LhBJ0%2BepE%2BfPpF12alTpyhp0qR0%2Fvx5IZY8eXJatGgRXb58mfz8%2FGjv3r1CbMWKFXTr1i1njIglQQpWrFy5MuJJrYP7XUCmvaayumXlypXia8wcA6B%2BQFDMmjZtSunTp6d79%2B7R2LFjqV%2B%2FfogJvC%2BTmDt3rpzDtWqRu3fv0oQJEyhz5sy4D%2FcjU7Umqcn2gsyaQDIenQVmd%2BLECercubMo1kCFPH36lHLlykVDhgyRmVWqVIl69OhBLVq0oOXLl8v7bGEMJO5dsGCB3MuTJw8tW7bMVmen5d6BzFm97%2FQt3HTs2DHCWoRYGTFihAy0Z88eybCyZctKisePH58qVqxIKVOmpGnTptHhw4cl7Q8dOkQ7d%2B4Ukg8fPhRyGocKKyDzUsnQvwisheKVJEkSatCgAVWrVk1mHBAQQP7%2B%2FnKeM2dOyaAsWbJQzZo1JTZ%2BQ7w%2BNmvgR1gBswBwjozA8dGjRxITO3bsoGbNmsngESJEABFBiBAhKGjQoJQpUyaaOXOmZJl%2Bp3pUr%2BqGy%2BFOFR9dgxCkWm3z5csncaHInTs3Zc2aFRCroABGihRJCPn6%2BgpgIVwXKlRIrFOqVCkhliNHDrGY1Zc3b16xXoUKFejmzZsaI%2BImXMGfXqwfmPmuXbsUcg2%2Fc%2BpLLGBNUsuECRMGKS1kQoUKJfeQMXDngQMHaP%2F%2B%2FVaHU6%2FEFIqtJbPDLgPOSLcxA0Jt2rTBzDBzGRyAizRmihYtCiDTdD37ZQGZsXaB1GUAwDlW4%2B3bt1Pp0qVlwMGDB9O2bdvkGkUPAQ3rwJXBggWjWbNmIYVBTN4fNWqUxBovOZic1Y1rmzhXQKaCXUWtoMpWrVpVSUjlRZ1AoUPcDB8%2BXKxQr149atu2raR%2B9OjRpXo%2FefKEVq1aRSVKlMD3SG8hYURTW29OAJkEqNTykAVrEMr5tWvXJKg3bdokJOAmyMKFC6Fclobp06dLv4POD4F66dIlateuHfXq1UuzRDJmzZo1qOTQjx4I7%2BkCa8kU8YEw40VEhMh2RY4cWQY7cuSIM3ak2OEZ2ge4r1atWnALLIH7qNoojij%2FmIhaQl0PPVhM8S6qNr5TImfZC2GFDGdSHrgJDdL69eu98LFWS1WI9kHdhVmht8E1CMKaBQsWpPnz5%2BsqjwCGFTVW1CVwsyQCbwYJdoDVWbo6G%2FF5ztixSpCGvXv3FhIQtKL4DCs2hGsLMk0XVUwKzxHsGqyAbVM9XEpQFK%2FrzlN7YBzjMB6ZzLINNayBJknvIUART1on0IgjhmAJbVnRoGt8qR6F12RRGTWK0zqVjFVgEskCLP%2Fo3JzbFhUdHFZD4Oo7yEZUdbSfSsTsoeDOUc6ttHO70oEBQRB7cAvZooPoaj5v3jy6fv06qeA54ungwYOWNLJQJ2KJQDaqV%2FSo4nRZR1RPPkXf4sbg1hpITTxDHKlgwGLFiuF9bcoFKrovN0RCWyP8ExlxGVfY%2Fkg%2FpDCUsLhwVPNj3XHuKM%2BcOaMdno03xJ%2F16YRf%2FUfCmiwEN1b5ObX3OPbMbm3EVMw%2BCwUSGYUbHkdVv8uo6AyLXxHnTrMBsvufFlF1H%2FricOHCadyoXEUdYYRTvcA%2FDswFKggjpAWnbUhuzEMDeIeX%2FXA842KcupM5WG%2BgsBmAHAqalxt2BPdTfmcxv1%2BPYy8uvuc1KhhvS0KzXjOOnGOcoAz%2F%2FwHhJgAbxX1etQAAAABJRU5ErkJggg%3D%3D)](https://smartnoise.org/) ![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)\n",
        "\n",
        "<a name=\"help\"/>\n",
        "\n",
        "### Don't get stuck - ask for help! ‚úã\n",
        "\n",
        "<sup><sub>Need some help? Reach us at:</sub></sup> </br>\n",
        "[![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://oblivious-community.slack.com)"
      ],
      "metadata": {
        "id": "r7ChW0YRGcXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Privacy Enhancing Technologies: A Quick Overview\n",
        "\n",
        "## What exactly is input privacy? üôà\n",
        "\n",
        "Input privacy techniques are a set of technologies which gaurentee how data is used and how it is kept secret _during_ computation. So you probably know about data being encrypted at rest (like in a database) and in transit (like when you use TLS/SSL connecting to any secure website), however these tools don't actually prove how your data is processed. That's a **big** gap in your ability to keep data secure throughout its lifecycle.\n",
        "\n",
        "Fortunately there are input privacy techniques like _homomorphic encryption_, _secure multiparty computation_ and _secure enclaves_ which can solve this challenge. In this hackathon you'll be connecting to a secure enclave which will transparently process your submissions and queries - so no one can cheat (not that any of you good people would üòá).\n",
        "\n",
        "> <font color=green>How enclaves work in 40 seconds:</font> (click through to YouTube)\n",
        "\n",
        "[![](https://i.ytimg.com/vi/9Z9FqtIr6go/hqdefault.jpg)](https://youtu.be/9Z9FqtIr6go)\n",
        "\n",
        "> <font color=pink>Example uses of multiparty computation in enclaves:</font>\n",
        "\n",
        "1.   Serve your AI model as an API in an enclave and give your users a gaurentee that you will never see their data.\n",
        "2.   Two companies can ask questions like \"How many shared customers do we have?\" without ever directly sharing any individual customer names with one another.\n",
        "3.   Train a deep learning model on medical images while gaurenteeing to hospitals that no person will ever be able to see the medical images being process.\n",
        "\n",
        "<br/>\n",
        "\n",
        "## And what about output privacy? üôä\n",
        "\n",
        "Output privacy is all about safely desemination information such that no one can reverse engineer the original input. The strongest definition for this is called **differential privacy** which has a clear and precise mathamatical definition (read more about it [here](https://en.wikipedia.org/wiki/Differential_privacy)). \n",
        "\n",
        "<br/>\n",
        "\n",
        "Watch a quick primer on differential privacy (3 mins) from NIST:\n",
        "\n",
        "[![](https://i.ytimg.com/vi/-JRURYTfBXQ/hqdefault.jpg)](https://youtu.be/-JRURYTfBXQ)\n",
        "\n",
        "<br/>\n",
        "\n",
        "For the sake of the hackathon, we'll keep the overview to a basic functional understanding (which we can fit into 3 bullet points):\n",
        "\n",
        "1.   Think of differential privacy as being based on you ability to geuss whether or not a person/entity/record is in a dataset or not based on the information shared. The more precise information shared, the easier it is to geuss. \n",
        "2.   The amount of information shared in any output is quantified be $\\epsilon$ (epsilon). If $\\epsilon=0$ then you will have absolutely no ability to geuss whether a person/entity/record exists in the data base. When it is $\\infty$ you would know with 100% confidence that the person/entity/record is or is not in the database.\n",
        "3.   Typically noise is intensionally added to results to gaurentee an upperbound on the $\\epsilon$ of privacy is lost. So your differentially private questions are going to recieve noisy answers. The more $\\epsilon$ you use, the more accurate the result. In real applications we would usually want $\\epsilon$ to be low (eg 1-10), but there is no magic rule to follow for this unfortunately.\n",
        "\n",
        "> <font color=pink>Example uses of differential privacy:</font>\n",
        "\n",
        "1.   National statistic beureaus want to share data and insights such as census information, yet do not want to leak informaiton that could be used to learn anything about a specific citizen.\n",
        "2.   I want to train a machine learning model on sensitive data but I intend on sharing the model, I can use differential privacy during the optimisation so the parameters learned can't give away specifics about the samples I've trained on.\n",
        "3.   Companies like Apple, Google and Facebook can use it to learn about how people use their devices without spying on everyones text messages and browsing habits.\n",
        "\n",
        "## How do I win? ü§ë\n",
        "\n",
        "This hackathon revolves around a realistic scenario - two parties (you and the oracle server) have two joinable sets of data with a shared ID. Even worse, you both want to create a machine learning/statistical model to make predictions but you have the inputs, they have the outputs. Alas, you don't trust one another to share your data directly, so you agree to only share your data through an enclave so neither party can see one anothers inputs. \n",
        "\n",
        "But of course, even enclaves will need to output _something_ to be useful. You both agree that only differentially private outputs will be diseminated from the enclave.\n",
        "\n",
        "But this is where you hit a wall... what questions should you ask from the oracle to best utalise the differential privacy $\\epsilon$?\n",
        "\n",
        "> The Metric for Success\n",
        "\n",
        "The competition will run just like any machine learning competition (kaggle, drivendata, etc) where model accuracy will determine the winner, but with the catch that your $\\epsilon$ will be subtracted off the accuracy like so:\n",
        "\n",
        "<font color=red>Score = accuracy - $\\epsilon / \\beta$</font>\n",
        "\n",
        "with $\\beta$ is a constant set to 500 to normalize the two values.\n",
        "\n",
        "![](https://c.tenor.com/g5luJt5ki30AAAAC/fortune-teller-crystall-ball.gif)\n",
        "\n",
        "RECAP of the rules:\n",
        "\n",
        "- The data of a machine learning challenge has been split into TRAIN and TEST sets.\n",
        "- You get TRAIN_X and TEST_X. But no TRAIN_Y and TEST_Y.\n",
        "- Note: All data columns (TRAIN and TEST sets) are categorical values in this competition.\n",
        "- You can send API calls to the enclave (oracle) to ask for the answer to SQL queries or to creat synthetic data (introduced a little later). \n",
        "- These requests will cost you $\\epsilon$, you specify how much you want to request on each call. These are logged by the enclave.\n",
        "- Every 5 mins you can make a geuss/prediction at TEST_Y, your all-time best geuss will be taken into account so you don't lose by trying.\n",
        "- The leaderboard is constructed by calculating you accuracy - $\\epsilon$/500. \n",
        "- At the end of the time limit which ever team wins will get swag and some personalized prized."
      ],
      "metadata": {
        "id": "HLNpXakSI7a7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## What can I actually do?\n",
        "\n",
        "### Submit Differentially Private SQL Queries:\n",
        "\n",
        "You can request aggregate SQL queries of the form:\n",
        "\n",
        "```sql\n",
        "SELECT c1, COUNT(labels) AS labels FROM comp.comp GROUP BY c1\n",
        "```\n",
        "\n",
        "or even \n",
        "\n",
        "```sql\n",
        "SELECT c1, c2, COUNT(labels) AS labels FROM comp.comp GROUP BY c1, c2\n",
        "```\n",
        "\n",
        "With every query you must also specify an $\\epsilon$ which must be between 0-10. The request will be handled by the [OpenDP SmartNoise-SQL](https://github.com/opendp/smartnoise-sdk/tree/main/sql) framework (a great initiative by a group at Harvard and Microsoft focussed solely on differential privacy).\n",
        "\n",
        "### Request Differentially Private Synthetic Data:\n",
        "\n",
        "Ok, hold-up... synthetic what now? I know we just through a new term at out once again. Synthetic data is a set of statistical/ML techniques that fits a model to input data to estimate the input distribution of the data. It then samples from that model to create to generate fake data that looks like the real data but without containing any actual datapoints from the original data set. \n",
        "\n",
        "Watch a few mins overview by Prof Jeff Heaton @ U. Washington (there are many great talks, but seen as you are under time pressure we thought we'd keep it brief):\n",
        "\n",
        "[![](https://i.ytimg.com/vi/yujdA46HKwA/hqdefault.jpg)](https://youtu.be/yujdA46HKwA)\n",
        "\n",
        "In this competition the oracle is willing to use all 4 synthetic data models provided by [OpenDP Smartnoise SynthData](https://github.com/opendp/smartnoise-sdk/tree/main/synth) (the same great team as mentioned earlier).\n",
        "\n",
        "The techniques are:\n",
        "\n",
        "1.   MWEM\n",
        "2.   DPCTGAN\n",
        "3.   PATECTGAN\n",
        "4.   MTS* \n",
        "\n",
        "*This one might be worth googling as it won the NIST DP Synthetic Data prize recently...\n",
        "\n",
        "When you query the oracle you must specify the model to use and the $\\epsilon$ to spend.\n",
        "\n"
      ],
      "metadata": {
        "id": "G-cFHMqUbfW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Sequential Composition Property of Differential Privacy\n",
        "Just like a real-life budget, you don't need to spend your privacy budget (denoted $\\epsilon$) all in one go ü§ë\n",
        "\n",
        "The sequential composition property of any differentially private mechanism means that if we make 2 differential private queries, each using a value of $\\epsilon_1$ and $\\epsilon_2$, then the overall budget used for running both of these queries sequentially is $\\epsilon_1 + \\epsilon_2$. In practice this means every time you either submit an SQL query or generate differentially private data, this adds to your budget. Beware that this decreases your final score, so use it wisely."
      ],
      "metadata": {
        "id": "tTfbB-53Rgsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "# READY, SET, GOOOOOOOOOO!"
      ],
      "metadata": {
        "id": "KjuF4U8qfIQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://c.tenor.com/PD4A2WhtUbcAAAAd/the-goon-dodgeball.gif)"
      ],
      "metadata": {
        "id": "XYdoL2JsIbCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### => Configuration **(Just run the cells below and enter your PASSCODE)**:\n",
        "\n",
        "Install the prerequists - the OBLV proxy by Oblivious that gaurentees the source code that the competition is running via cryptographic attestation: (run cell below)"
      ],
      "metadata": {
        "id": "I17U6qERhlVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# downlaod the patch for glibc on ubuntu 18.04 (which colab runs) and the OBLV proxy \n",
        "wget https://oblv.io/gcolab-quickstart-v0-2-0 -O oblv-gcolab-patch-v0.2.0.zip -q\n",
        "# unzip it\n",
        "unzip -qq oblv-gcolab-patch-v0.2.0.zip \n",
        "# give exec permission to the installer\n",
        "chmod 755 ./oblv-gcolab-patch-v0.2.0/install.sh\n",
        "# install\n",
        "./oblv-gcolab-patch-v0.2.0/install.sh > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "3o6SXx-diMVz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and download your public/private key-pairs by replacing the \\<team_secret\\> below with your team's name provided at the beginning of the hackathon. You will need these in order to connect to the sandbox/real competition enclaves and they will be used to authenticate your teams submissions:\n"
      ],
      "metadata": {
        "id": "UHxudKnPirUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "#Participants private and public keys\n",
        "wget https://google-colab-quickstart.s3.eu-west-2.amazonaws.com/participant_keys/<team_secret>/oblv_public.der\n",
        "\n",
        "wget https://google-colab-quickstart.s3.eu-west-2.amazonaws.com/participant_keys/<team_secret>/oblv_private.der"
      ],
      "metadata": {
        "id": "m3hScScbj7kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, run the OBLV proxy and you are good to go ‚úå"
      ],
      "metadata": {
        "id": "os5-skRreN22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "## Proxy for Competition:\n",
        "\n",
        "# This runs the OBLV enclave proxy for you on http://localhost:3030. \n",
        "# You sending requests to this address will get encrypted sent to the enclave.\n",
        "#   - The PCR codes are what gaurentee what is running inside the enclave\n",
        "#   - If you were to fork https://github.com/ObliviousAI/Synthetic-Data-Competition\n",
        "#     and deploy it to an enclave through oblivious.ai, you'll get the exact same\n",
        "#     pcr codes\n",
        "#   - The signatures that gaurentee this are from the underlying infrastructure \n",
        "#     (in our case AWS Nitro Enclaves) \n",
        "oblv connect \\\n",
        "--pcr0 bfc9060de5425cfad0ebe95b101169228bd83f4a0685def23588c71f0326967581f8d379d5c88e8999441d48ea165a8e \\\n",
        "--pcr1 bcdf05fefccaa8e55bf2c8d6dee9e79bbff31e34bf28a99aa19e6b29c37ee80b214a414b7607236edf26fcb78654e63f \\\n",
        "--pcr2 0d054f92d9f6e355d84d18252f69eb6619d1d6992b4b397ef9b8a56ef4244738921760d0206071012af69337322f10a6 \\\n",
        "--private-key \"/content/oblv_private.der\" \\\n",
        "--public-key \"/content/oblv_public.der\" \\\n",
        "--url https://hacka-appli-17oc1lv2gu2bn-1671279079.enclave.oblivious.ai/ --port 443 --lport 3030 -c\n",
        "\n"
      ],
      "metadata": {
        "id": "38XEY-bzeZbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash --bg\n",
        "## Proxy for Sandbox:\n",
        "\n",
        "# This is doing the exact same as the last cell, but this enclave is running on \n",
        "# a different server (ip address) as is running the sandbox.\n",
        "# We are running it on 4040 to differentiate the sandbox from the real competition.\n",
        "#\n",
        "# (Note the PCR codes are the same!!)\n",
        "oblv connect \\\n",
        "--pcr0 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 \\\n",
        "--pcr1 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 \\\n",
        "--pcr2 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 \\\n",
        "--private-key \"/content/oblv_private.der\" \\\n",
        "--public-key \"/content/oblv_public.der\" \\\n",
        "--url https://sandb-appli-cj7mv01z9l4j-1558941647.enclave.oblivious.ai/ \\\n",
        "--port 443 --lport 4040 -c"
      ],
      "metadata": {
        "id": "-iHCZeRagbkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sandbox üèúÔ∏è\n",
        "\n",
        "Below we will give you an example of all of the data SANDBOX_TRAIN_X, SANDBOX_TRAIN_Y, SANDBOX_TEST_X and SANDBOX_TEST_Y and an oracle to play around with. This data is not part of the competition, it's simply so you can experiment and find your strategy ü§ì\n",
        "\n",
        "> What's in the sandbox?\n",
        "\n",
        "- SANDBOX_TRAIN_X: the inputs for training in the sandbox\n",
        "- SANDBOX_TRAIN_Y: the outputs for training in the sandbox\n",
        "- SANDBOX_TEST_X: the inputs for testing in the sandbox\n",
        "- SANDBOX_TEST_Y: the outputs for testing in the sandbox\n",
        "- SANDBOX_ENCLAVE_URL: the URL for the sandboxed experiments (a dummy oracle)\n",
        "\n",
        "‚ÑπÔ∏è *Use the sand box to create synthetic data and make SQL queries. It's not going to effect your score and you can try to figure out what \"works\". Then once you've a plan in place, copy it down into the real competition below.*"
      ],
      "metadata": {
        "id": "N_Bip-5mfXmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import json\n",
        "\n",
        "COLNUMS = 8\n",
        "\n",
        "def validate_cols(_cols):\n",
        "  for c in _cols:\n",
        "    assert c in ([\"col\"+str(i+1) for i in range(COLNUMS)] + [\"labels\"])\n",
        "\n",
        "def validate_funcs(_func):\n",
        "  for f in _func:\n",
        "    print(f)\n",
        "    assert f[0] in [\"AVG\", \"VAR\", \"COUNT\"]\n",
        "    assert f[1] in ([\"col\"+str(i+1) for i in range(COLNUMS)] + [\"labels\"])\n",
        "\n",
        "def validate_eps(_eps):\n",
        "  assert _eps > 0\n",
        "  assert _eps <= 10\n",
        "\n",
        "def validate_sd_method(_method):\n",
        "  assert _method in [\"MWEM\", \"MST\", \"DPCTGAN\", \"PATECTGAN\"]\n",
        "\n",
        "def sql_params(cols, func, eps):\n",
        "  funcs = ', '.join([f\"{f[0]}({f[1]}) AS labels_{f[0]}\" for f in func])\n",
        "  cols = ', '.join(cols)\n",
        "  return {\n",
        "      \"eps\": eps,\n",
        "      \"query_str\": f\"SELECT {cols}, {funcs} FROM comp.comp GROUP BY {cols}\"\n",
        "  }\n",
        "    \n",
        "def generate_sql_query(url, cols, func, eps) -> dict:\n",
        "  validate_cols(cols)\n",
        "  validate_funcs(func)\n",
        "  validate_eps(eps)\n",
        "  out = {\n",
        "      \"url\": url+'query',\n",
        "      \"params\": sql_params(cols, func, eps)\n",
        "  }\n",
        "  return out\n",
        "\n",
        "\n",
        "def parse_synth_data_result(result) -> pd.DataFrame:\n",
        "    if result.status_code != 200:\n",
        "        print(f\"Request recieved an error code: {result.status_code}\")\n",
        "        return None\n",
        "    data = result.content.decode('utf8')\n",
        "    df = pd.read_csv(io.StringIO(data))\n",
        "    return df \n",
        "    \n",
        "def parse_sql_query_result(response) -> pd.DataFrame:\n",
        "  if response.status_code != 200:\n",
        "    print(f\"Request recieved an error code: {response.status_code}\")\n",
        "    return None\n",
        "  res = json.loads(response.content.decode('utf-8'))\n",
        "  return pd.DataFrame(res[1:], columns=res[0])\n",
        "\n",
        "def generate_synth_data(url, synth_model, eps) -> dict:\n",
        "  validate_sd_method(synth_model)\n",
        "  validate_eps(eps)\n",
        "  out = {\n",
        "      \"url\": url+'synthesize',\n",
        "      \"params\": {\n",
        "          \"model\": synth_model,\n",
        "          \"eps\": eps\n",
        "      }\n",
        "  }\n",
        "  return out\n",
        "\n",
        "  def parse_synth_data_result(result) -> pd.DataFrame:\n",
        "    if result.status_code != 200:\n",
        "        print(f\"Request recieved an error code: {result.status_code}\")\n",
        "        return None\n",
        "    data = result.content.decode('utf8')\n",
        "    df = pd.read_csv(io.StringIO(data))\n",
        "    return df "
      ],
      "metadata": {
        "id": "advFC_XmsTSD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "SANDBOX_ENCLAVE_URL = \"http://localhost:4040/\"\n",
        "\n",
        "cols = [\"col1\", \"col2\"] # allowed 1 column like \"col1\" or multiple like \"col2, col3\" (the comma is important)\n",
        "func = [[\"AVG\", \"labels\"]] # allowed options are: [\"AVG\", \"VAR\", \"COUNT\"....] followed by a column name\n",
        "eps = 4.1 #between 0-10\n",
        "\n",
        "response = requests.get(**generate_sql_query(SANDBOX_ENCLAVE_URL, cols, func, eps))\n",
        "print(response.content)\n",
        "result = parse_sql_query_result(response)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "_SGBa9VsQmA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaiable models : [MST, DPCTGAN, MWEM, PATECTGAN]\n",
        "model = \"MST\"\n",
        "eps = 4.1 #between 0-10\n",
        "response= requests.get(**(generate_synth_data(SANDBOX_ENCLAVE_URL, model, eps)))\n",
        "synth_result_df = parse_synth_data_result(response)\n",
        "print(synth_result_df)"
      ],
      "metadata": {
        "id": "SXxidPJgEyoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some template functions for seaborn visualisations\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#at this stage assuming that participants have generated some synthetic data and want to start exploring relationships. \n",
        "#Just wondering whether it would make sense to allow them to query dp-histograms behind the API (or maybe we give end-to-end examples of how to generate histograms from the SQL queries either)\n",
        "\n",
        "#distplot and histogram always useful\n",
        "def show_distribution(dataframe: pd.DataFrame, count_column: \"count column for histogram\", no_bins= \"auto\", col= None, kind = \"hist\"):\n",
        "  '''\n",
        "  Given a dataframe, displays the distribution of a column (count_column). If _col is provided, it dentfines subsets to plot on different facets.\n",
        "  To plot a simple histogram, only specify _dataframe and _x\n",
        "  To plot a histogram conditioned on another column, specify _col also (be careful that col doesn't contain too many categories) \n",
        "  @param dataframe: the dataframe containing the data to be plotted\n",
        "  @param count_column: the column which will be used to generate the counts for the histogram (if col specified, these counts will be conditional on col)\n",
        "  @param no_bins: the number of bins to use for the histogram\n",
        "  @param kind: the kind of plot (use \"hist\" for discrete histogram, \"kde\" for density)\n",
        "  '''\n",
        "  assert isinstance(dataframe, pd.DataFrame)\n",
        "  assert count_column in dataframe.columns, f\"{count_column} is not present in the dataframe\"\n",
        "  if col: \n",
        "    assert col in dataframe.columns, f\"{col} is not present in the dataframe\"\n",
        "  if kind == \"hist\":\n",
        "    sns.displot(dataframe, x = count_column, col = col, bins = no_bins, kind = kind)\n",
        "  elif kind == \"kde\":\n",
        "    sns.displot(dataframe, x = count_column, col = col, kind = kind)\n",
        "#pretend we generated the following synthetic data, y is independent variable\n",
        "test_histogram_df = pd.DataFrame(columns=\n",
        "  [\n",
        "    \"col1\",\n",
        "    \"col2\",\n",
        "    \"col3\",\n",
        "   \"y\"\n",
        "    \n",
        "  ],\n",
        "  data = np.array([\n",
        "    #generally increasing with some noise\n",
        "    [\n",
        "      int(i + random.randint(-8,8)) for i in range(20)\n",
        "    ],\n",
        "    #generally decreasing with some noise\n",
        "    [\n",
        "     int(-i + random.randint(-8,8)) for i in range(20) \n",
        "    ],\n",
        "    #a bunch of randomly generated binary as noise\n",
        "    [True if random.random() > 0.5 else False for i in range(20)],\n",
        "        [\n",
        "     True for i in range(15)\n",
        "    ] + [False for i in range(5)]\n",
        "\n",
        "  ]).T\n",
        ")\n",
        "#overview of the distribution for col1\n",
        "show_distribution(synth_result_df, \"col1\", no_bins = 5)\n",
        "\n",
        "#overview of the distribution conditioning on True/False y col\n",
        "show_distribution(synth_result_df, \"col1\", no_bins = 5, col = \"labels\")\n",
        "\n",
        "#overview of the distribution conditioning on True/False y col\n",
        "show_distribution(synth_result_df, \"col1\", col = \"labels\", kind = \"kde\")\n",
        "\n",
        "#col3 is binary, can see that the response is approx. uniformly distributed in relation to col3\n",
        "show_distribution(synth_result_df, \"col3\", no_bins = 5, col = \"labels\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-CYhJUKUXwtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_pairplot(dataframe, diag_kind = \"auto\", hue = 'labels', vars = None):\n",
        "  '''\n",
        "  By default plot will be coloured (hue) using the response (independent) variable. Set hue=None to avoid this\n",
        "  '''\n",
        "  if vars:\n",
        "    assert set(vars).intersection(dataframe.columns) == set(vars), f\"{set(vars).difference(set(dataframe.columns))} are not columns in the dataframe\"\n",
        "  sns.pairplot(dataframe, vars = vars, diag_kind = diag_kind, hue = hue)\n",
        "\n",
        "#At a glance we can use this function to see how the independent variable is affected by col1, col2 and col3 \n",
        "#on the diagonals, we get a univariate density plot (i.e. the top left corner, we see a density plot of col1 coloured by y)\n",
        "#off the diagonals, we see a density plot of each pair of columns, again colored by hue. This gives a sense of how the variables related to each other. \n",
        "#in these case we can see as col1 increases, col2 also increases. \n",
        "show_pairplot(synth_result_df)\n",
        "\n",
        "#instead of densities we can also look at histograms instead\n",
        "show_pairplot(synth_result_df, diag_kind = \"hist\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HojHWqn_7WW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some simple ML models"
      ],
      "metadata": {
        "id": "WY9QsYjWEdEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "#train a naive linear model with all variables\n",
        "#should split this into train and test (assuming we have enough data in the sandbox)\n",
        "x_cols = synth_result_df[synth_result_df.columns.difference(['labels'])]\n",
        "y_true = synth_result_df['labels']\n",
        "model = LogisticRegression().fit(x_cols, y_true)\n",
        "y_pred = model.predict(x_cols)\n",
        "#See how the model does on our dataset\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "9RFqUb5y8VjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we know from looking at the plots above col3 doesn't predict y well, let's see what happens if we exclude it\n",
        "x_cols_without_col3 = synth_result_df[synth_result_df.columns.difference(['labels', \"col3\"])]\n",
        "model_without_col3 = LogisticRegression().fit(x_cols_without_col3, y_true)\n",
        "y_pred_without_col3 = model_without_col3.predict(x_cols_without_col3)\n",
        "#See how the model does on our dataset\n",
        "accuracy_score(y_true, y_pred_without_col3)\n"
      ],
      "metadata": {
        "id": "gl0RyqEBJvy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic decision tree code\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree_classifier = DecisionTreeClassifier(random_state = 0)\n",
        "tree_classifier.fit(x_cols, y_true)\n",
        "y_pred = tree_classifier.predict(x_cols)\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "XmtM3gbSPmwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Real Competition ‚öîÔ∏è\n",
        "\n",
        "This is the real deal now. Proceed with caution! ‚ö†Ô∏è\n",
        "\n",
        "> What's in the real competition section?\n",
        "\n",
        "- TRAIN_X: the inputs for training \n",
        "- TEST_X: the inputs for testing \n",
        "- ENCLAVE_URL: the URL for the competiions enclave\n"
      ],
      "metadata": {
        "id": "emvcrhJugm-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "COMPETITION_URL = \"http://localhost:3030\"\n",
        "\n",
        "params = {\n",
        "    \"eps\": 0.4, #between 0-10\n",
        "    \"query_str\": \"SELECT col1, COUNT(labels) AS labels FROM comp.comp GROUP BY col1\"\n",
        "}\n",
        "\n",
        "response = requests.get(COMPETITION_URL+'/query', params=params)\n"
      ],
      "metadata": {
        "id": "QR5AtGVvIY74"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "id": "RdGEDcGdtiOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submit data generated from your model - submission will be ranked as per the accuracy of this file along with epsilon usage\n",
        "import requests\n",
        "\n",
        "COMPETITION_URL = \"http://localhost:3030\"\n",
        "\n",
        "\n",
        "\n",
        "response = requests.post(COMPETITION_URL+'/submit', files = {\"file\": open(\"submit_example.csv\", \"rb\")})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "zonrEv6xuNo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In case any user files are to be uploaded to collab\n",
        "from google.colab import files\n",
        "import io\n",
        "uploaded = files.upload()\n",
        "\n",
        "for file_name, file_content in uploaded.items():\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(file_content)"
      ],
      "metadata": {
        "id": "y0uQ3a0yDziN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Submission Score\n",
        "import requests\n",
        "COMPETITION_URL = \"http://localhost:3030\"\n",
        "\n",
        "response = requests.get(COMPETITION_URL+'/score')"
      ],
      "metadata": {
        "id": "jcywdWT3gOS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Submission Accuracy\n",
        "import requests\n",
        "COMPETITION_URL = \"http://localhost:3030\"\n",
        "\n",
        "response = requests.get(COMPETITION_URL+'/accuracy')"
      ],
      "metadata": {
        "id": "j5OOL9ABgbce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}